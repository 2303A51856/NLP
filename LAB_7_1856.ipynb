{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjSL59EVatdds1udWvRBoO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51856/NLP/blob/main/LAB_7_1856.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujj04SjAqJZG",
        "outputId": "26d238a2-a344-4894-9d96-78a953941f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          clean_text  is_depression\n",
            "0  we understand that most people who reply immed...              1\n",
            "1  welcome to r depression s check in post a plac...              1\n",
            "2  anyone else instead of sleeping more when depr...              1\n",
            "3  i ve kind of stuffed around a lot in my life d...              1\n",
            "4  sleep is my greatest and most comforting escap...              1\n",
            "Index(['clean_text', 'is_depression'], dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/depression_dataset_reddit_cleaned.csv\")\n",
        "\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "\n",
        "\n",
        "texts = df['clean_text'].astype(str)\n",
        "labels = df['is_depression']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['clean_text'] = texts.apply(preprocess_text)"
      ],
      "metadata": {
        "id": "LN5MkbAOu57q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_text'], labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "UmoNnvGTvB3M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tfidf_uni = TfidfVectorizer(ngram_range=(1,1), max_features=5000)\n",
        "X_train_uni = tfidf_uni.fit_transform(X_train)\n",
        "X_test_uni = tfidf_uni.transform(X_test)\n",
        "\n",
        "\n",
        "tfidf_bi = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "X_train_bi = tfidf_bi.fit_transform(X_train)\n",
        "X_test_bi = tfidf_bi.transform(X_test)\n",
        "\n",
        "\n",
        "tfidf_tri = TfidfVectorizer(ngram_range=(1,3), max_features=5000)\n",
        "X_train_tri = tfidf_tri.fit_transform(X_train)\n",
        "X_test_tri = tfidf_tri.transform(X_test)\n"
      ],
      "metadata": {
        "id": "8vEa67AkvFCn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "def build_ann(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "ann_uni = build_ann(X_train_uni.shape[1])\n",
        "history_uni = ann_uni.fit(X_train_uni, y_train, validation_data=(X_test_uni, y_test),\n",
        "                          epochs=5, batch_size=64, verbose=1)\n",
        "\n",
        "\n",
        "ann_bi = build_ann(X_train_bi.shape[1])\n",
        "history_bi = ann_bi.fit(X_train_bi, y_train, validation_data=(X_test_bi, y_test),\n",
        "                        epochs=5, batch_size=64, verbose=1)\n",
        "\n",
        "\n",
        "ann_tri = build_ann(X_train_tri.shape[1])\n",
        "history_tri = ann_tri.fit(X_train_tri, y_train, validation_data=(X_test_tri, y_test),\n",
        "                          epochs=5, batch_size=64, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PIQwiz0vLc3",
        "outputId": "1a12c919-ed6f-4c65-a354-496c30ec67ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.8153 - loss: 0.5122 - val_accuracy: 0.9657 - val_loss: 0.1056\n",
            "Epoch 2/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9752 - loss: 0.0749 - val_accuracy: 0.9599 - val_loss: 0.1109\n",
            "Epoch 3/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9896 - loss: 0.0334 - val_accuracy: 0.9606 - val_loss: 0.1194\n",
            "Epoch 4/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.9970 - loss: 0.0174 - val_accuracy: 0.9560 - val_loss: 0.1367\n",
            "Epoch 5/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9975 - loss: 0.0122 - val_accuracy: 0.9528 - val_loss: 0.1567\n",
            "Epoch 1/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.7241 - loss: 0.5168 - val_accuracy: 0.9651 - val_loss: 0.1054\n",
            "Epoch 2/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.9742 - loss: 0.0754 - val_accuracy: 0.9502 - val_loss: 0.1192\n",
            "Epoch 3/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9951 - loss: 0.0264 - val_accuracy: 0.9554 - val_loss: 0.1262\n",
            "Epoch 4/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9972 - loss: 0.0182 - val_accuracy: 0.9541 - val_loss: 0.1423\n",
            "Epoch 5/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.9970 - loss: 0.0118 - val_accuracy: 0.9522 - val_loss: 0.1579\n",
            "Epoch 1/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.8007 - loss: 0.5095 - val_accuracy: 0.9644 - val_loss: 0.1055\n",
            "Epoch 2/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9791 - loss: 0.0654 - val_accuracy: 0.9586 - val_loss: 0.1125\n",
            "Epoch 3/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9925 - loss: 0.0297 - val_accuracy: 0.9573 - val_loss: 0.1249\n",
            "Epoch 4/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.9969 - loss: 0.0136 - val_accuracy: 0.9535 - val_loss: 0.1466\n",
            "Epoch 5/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9981 - loss: 0.0092 - val_accuracy: 0.9470 - val_loss: 0.1674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM\n",
        "\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "def build_lstm():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "lstm_model = build_lstm()\n",
        "history_lstm = lstm_model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test),\n",
        "                              epochs=5, batch_size=64, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mStgarTKvraH",
        "outputId": "8979caf9-e27a-498f-ba84-3870a817b257"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 314ms/step - accuracy: 0.8505 - loss: 0.4302 - val_accuracy: 0.9683 - val_loss: 0.0940\n",
            "Epoch 2/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 308ms/step - accuracy: 0.9740 - loss: 0.0764 - val_accuracy: 0.9735 - val_loss: 0.0794\n",
            "Epoch 3/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 335ms/step - accuracy: 0.9841 - loss: 0.0479 - val_accuracy: 0.9670 - val_loss: 0.0902\n",
            "Epoch 4/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 333ms/step - accuracy: 0.9879 - loss: 0.0344 - val_accuracy: 0.9638 - val_loss: 0.0911\n",
            "Epoch 5/5\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 327ms/step - accuracy: 0.9924 - loss: 0.0210 - val_accuracy: 0.9722 - val_loss: 0.0930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"ANN Unigram:\", ann_uni.evaluate(X_test_uni, y_test, verbose=0))\n",
        "print(\"ANN Bigram :\", ann_bi.evaluate(X_test_bi, y_test, verbose=0))\n",
        "print(\"ANN Trigram:\", ann_tri.evaluate(X_test_tri, y_test, verbose=0))\n",
        "\n",
        "\n",
        "print(\"LSTM:\", lstm_model.evaluate(X_test_pad, y_test, verbose=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUXOLWRPwyMq",
        "outputId": "d25864e8-6e9e-4dc6-e767-336ba1f2d8e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN Unigram: [0.15670284628868103, 0.9528118968009949]\n",
            "ANN Bigram : [0.1579189896583557, 0.9521654844284058]\n",
            "ANN Trigram: [0.16744530200958252, 0.9469941854476929]\n",
            "LSTM: [0.09299895912408829, 0.9722042679786682]\n"
          ]
        }
      ]
    }
  ]
}